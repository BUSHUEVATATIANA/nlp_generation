# nlp_generation
Repository for NLP generation tasks\
Данный проект представялет собой чат-бот, который должен вести диалог как определенный персонаж сериала, имитируя стиль и манеру конкретного персонажа сериала.\
Я выбрала Чендлера из сериала Друзья.\
 # ***Этапы работы над проектом:***
 ## 1. Сбор данных
Были получны данные с сайта [Kaggle](https://www.kaggle.com/datasets/amandam1/friends-scripts "Cсылка на репозиторий датасета")\
Данные представляют собой датасет, состоящий из пяти столбцов:
* season_number
* episode_number
* episode_name
* character
* line

и 61256 строк. 
## 2. Предобработка полученных данных


В процессе предобработки данных были удалены пропущенные значения и дубликаты. Количество строк сократилось до 56404 и для решения задачи необходимы только два столбца:character и line. \
В процессе работы над задачей, было замечено, что некоторые фразы героя разбиты на две строки, следующие друг за другом, данные строки необходимо объединить. Был добавлен столбец group, который увеличивается, если character меняется.\
Данные были сгрупированы по character и group, объединяя строки line. Количество строк сократилось до 53174.\
Далее, в процессе работы над проектом был сделан вывод, что для решения задачи необходимы только фразы героя и фразы следующие до фразы героя (по которым мы будем искать наименьшее косинусное расстояние).\
Для этого был создан словарь, где ключ: реплика героя, значение: реплика до реплики героя.\
Как результат получен датасет с двумя столбцами:
* phrase before
* chandler's phrase\

В котором осталось 7173 строк.\

В качестве последнего шага предобработки к столбцу  'phrase before' была применена функция clean_data, в которой удираются лишние символы, эмоджи,знаки пунктуации, лишние пробелы, разворачиваются сокращения, преобразование символов с диакритическими знаками к ASCII-символам, удаление стоп слов.\
Колонка chandler's phrase осталась без изменений, так как в модель она поступать не будет, она нужна для ответа на вопрос пользователя.

### 3. Посторение модели

Перед началом работы над построением модели были посторены гистограмы распределения длины текстов в датафрейме:\
Для столбца clean_phrase before:\
![Гистограмма для phrase before](/images/hist for phrase before.png)\
Для столбца chandler's phrase:\
![Гистограмма для phrase before](/images/hist for chandler's phrase.png)\

Для безлайна была выбрана модель distillbert. Но в процессе работы была найдена предобученая модель `sentence-transformers/all-MiniLM-L6-v2`, которую я использовала в конечном решении. В результате работы класса ModelSearchEngine была выстроен алгоритм, принимающий на вход модель, токенайзер и датафрейм.  Класс содержит:
* метод предобработки текста(`preprocess`): очистка данных и токенизация;
* приватный метод построения эмбеддингов текста(`_embed_cls`), принимающая на вход токенизированный текст;
* приватный метод инициализирует систему поиска документов(`_init_retriever`), где сначала сохраняются переданные модель и токенизатор как атрибуты объекта, а затем создаёт базу векторов представлений всех документов из базы данных, где каждый документ последовательно обрабатывается методом `preprocess`, преобразуется в векторное представление с помощью метода `_embed_cls` , все векторы сохраняются в массиве `self.base` Далее с помощью библиотеки Annoy созданные векторные представления сохраняются в файл `annoy_index.ann`. Это поможет эффективно искать ближайших соседей и ускорит интерфейс модели;
* метод `retrieve` , который обращается к методу `_embed_cls` для получения эмбеддинга запроса пользователя;
* метод `retrieve_documents ` выполняет поиск наиболее релевантных документов в базе данных, где для входного запроса сначала создаёт векторное представление с помощью метода `retrieve`, затем вычисляет косинусную схожесть между запросом и всеми документами в базе, сортирует документы по убыванию схожести и возвращает индексы top_k наиболее релевантных документов в виде списка, что позволяет эффективно находить тексты, наиболее близкие по содержанию к поисковому запросу. 
Данный метод использовался только в бейзлане решения, так как для быстрой работы алгоритма на интерфейсе было принято решения использовать механизм Annoy, так как:
    + он очень быстрый при поиске
    + требует низких затрат памяти
    + оптимизирован для работы с большими объемами данных
    + обеспечивает компромисс между скоростью и точностью


* метод `_init_inverted_index` создает словарь с пронумерованым датасетом;

* метод `display_relevant_docs` получает поисковый запрос и возвращает список индексов наиболее релевантных документов из базы данных, где сначала вызывает метод `retrieve_documents` для получения индексов top_k наиболее подходящих документов, а затем использует обратный индекс (`inverted_index`) для преобразования этих индексов в их исходные идентификаторы, что позволяет эффективно извлекать и возвращать идентификаторы найденных документов в виде списка целых чисел.\
Данный метод так же использован только в безлайне проекта.

Как результат данного этапа мы получили файл с векторными представлениями столбца 'phrase before', который можно использовать\
для создания скрипта интерфейса;

Ноутбук c кодом: ***Chat_bot_train.ipynb*** 

### 4. Упаковка решения в виде web-сервиса
С помощью библиотеки Flask был создан веб-сервис, который работает на localhost:5555.\
Для создания интерфейса было найдено готовое html-решение: ***web_client/app/templates/chat.html***. Он так же поддерживает\
возможность озвучки ответа бота (но она не очень хорошо работает).\
Был написан скрипт ***app.py*** для обработки поступающих запросов, который создает эмбеддинги запроса и ищет ближайших соседей с помощью метода `get_nns_by_vector` из библиотеки Annoy.\
В скрипт не была добавлена функция clean_text, так как ответ был одинаковый без нее и с ней.
Созданный сервис был упакован в Dockerfile, созданный образ добавлен в Docker Hub и чтобы его загрузить нужно выполнить следующие команды терминале:
```python
docker login
docker pull tatiana1234/flask_image
docker run -it --rm --name=flask_container -p=5555:5555 tatiana1234/flask_image
```
### 5.Вывод
Так как данное решение представляет собой поиск наиболее схожей к запросу пользователя фразы из датасета, а ответ это всего лишь фраза, сказанная героем после схожей фразы, выданные ответы не всегда понятны без контекста разговора. 







